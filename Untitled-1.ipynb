{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "import wandb\n",
    "import gc\n",
    "import os\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "os.environ[\"WANDB_API_KEY\"] = 
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# Memory management and device settings\n",
    "def setup_environment():\n",
    "    # Clear any existing cached memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Enable memory efficient optimizations\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    # Set memory allocation settings\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.set_per_process_memory_fraction(0.9)  # Use up to 90% of GPU memory\n",
    "        torch.cuda.set_device(0)  # Set primary GPU if multiple available\n",
    "        \n",
    "    # Enable TF32 tensor cores on Ampere GPUs\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "setup_environment()\n",
    "\n",
    "# Function to move data to appropriate device\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(csv_file, root_dir, batch_size, transform):\n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, csv_file, root_dir, transform=None):\n",
    "            self.annotations = pd.read_csv(csv_file)\n",
    "            self.root_dir = root_dir\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.annotations)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            \n",
    "            img_name = f\"{self.root_dir}/{self.annotations.iloc[idx, 8]}\"\n",
    "            try:\n",
    "                image = Image.open(img_name).convert(\"L\").convert(\"RGB\")\n",
    "            except:\n",
    "                print(img_name)\n",
    "            label1 = torch.tensor([self.annotations.iloc[idx, 3], abs(1 - self.annotations.iloc[idx, 3])])* self.annotations.iloc[idx, 7].squeeze()\n",
    "            label2 =  self.annotations.iloc[idx, 3]\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            return image, (label1, label2)\n",
    "\n",
    "    dataset = CustomDataset(csv_file=csv_file, root_dir=root_dir, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers = 8)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "# Model Modification\n",
    "def modify_resnet50(device):\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "\n",
    "    # Modify the fully connected layer to output two sets of classes\n",
    "    class CustomResNet50(nn.Module):\n",
    "        def __init__(self, original_model):\n",
    "            super(CustomResNet50, self).__init__()\n",
    "            self.features = nn.Sequential(*list(original_model.children())[:-1])\n",
    "            for param in self.features.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            self.fc1 = nn.Sequential(\n",
    "                nn.Linear(num_ftrs, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(512, 2),\n",
    "                nn.Sigmoid()\n",
    "                )\n",
    "            \n",
    "            self.fc2 = nn.Sequential(\n",
    "                nn.Linear(num_ftrs, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(512, 512),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 1),\n",
    "                nn.Sigmoid()\n",
    "                )\n",
    "            \n",
    "            self.linear = nn.Linear(258, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.features(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            out1 = (self.fc1(x)-0.5)*8\n",
    "            out2 = self.fc2(x)\n",
    "            out2_expanded = torch.cat((out2, abs(1 - out2)), dim=1)\n",
    "            out1 = out1 * out2_expanded\n",
    "            #out2 = nn.Tanh()(self.linear(torch.cat((x, out1), dim=1)))\n",
    "            return out1, out2   # allows for distance to be between -4 and 4\n",
    "        \n",
    "        def freeze(self, freeze = True):\n",
    "            freeze = not freeze # Invert the freeze parameter to transorm it in require grad\n",
    "            for param in self.features.parameters():\n",
    "                param.requires_grad = freeze\n",
    "\n",
    "                \n",
    "        def freeze_fc2(self, freeze = True):\n",
    "            freeze = not freeze # Invert the freeze parameter to transorm it in require grad\n",
    "            for param in self.fc2.parameters():\n",
    "                param.requires_grad = freeze\n",
    "\n",
    "    model = CustomResNet50(model).to(device)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Parameters\n",
    "csv_file = 'clean_images_data.csv'\n",
    "root_dir = './../PrimateRobustness'\n",
    "batch_size = 100\n",
    "\n",
    "# Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Model\n",
    "model = nn.DataParallel(modify_resnet50(device))\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 30\n",
    "initial_lr = 0.001\n",
    "min_lr = 0.0001\n",
    "decay_factor = 0.1\n",
    "patience = 3 \n",
    "\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.MSELoss()\n",
    "optimizer1 = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "optimizer2 = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "\n",
    "lr_scheduler1 = optim.lr_scheduler.ReduceLROnPlateau(optimizer1, mode='min', factor=decay_factor, patience=patience, min_lr=min_lr)\n",
    "lr_scheduler2 = optim.lr_scheduler.ReduceLROnPlateau(optimizer2, mode='min', factor=decay_factor, patience=patience, min_lr=min_lr)\n",
    "learning_rate = initial_lr\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"two_tailed\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"initial_learning_rate\": learning_rate,\n",
    "    \"min_learning_rate\": min_lr,\n",
    "    \"epochs\": num_epochs,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DataLoader\n",
    "dataloader = get_dataloader(csv_file, root_dir, batch_size, transform)\n",
    "\n",
    "print('start_training loop', csv_file)\n",
    "# Training loop\n",
    "scaler = GradScaler(device)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    batch_num = 0\n",
    "    for images, (labels1, labels2) in dataloader:\n",
    "        batch_num += 1\n",
    "        # Forward pass\n",
    "        with autocast(device):\n",
    "            outputs = model(images.to(device))\n",
    "            outputs1 = outputs[0]\n",
    "            outputs2 = outputs[1].squeeze()\n",
    "            \n",
    "            loss1 = criterion1(outputs1.float(), labels1.float().to(device))\n",
    "            loss2 = criterion2(outputs2.float(), labels2.float().to(device))\n",
    "        #if epoch<1:\n",
    "        model.module.freeze(False)\n",
    "        optimizer1.zero_grad()\n",
    "        scaler.scale(loss1).backward(retain_graph = True)\n",
    "        scaler.step(optimizer1)\n",
    "        scaler.update()\n",
    "        lr_scheduler1.step(loss1.item())\n",
    "        model.module.freeze(True)\n",
    "        optimizer2.zero_grad()\n",
    "        scaler.scale(loss2).backward()\n",
    "        scaler.step(optimizer2)\n",
    "        scaler.update()\n",
    "        lr_scheduler2.step(loss2.item())\n",
    "        current_loss = loss1.item() + loss2.item()\n",
    "        running_loss += current_loss\n",
    "        \n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        wandb.log({\"loss1\": loss1.item(), \"loss2\": loss2.item()})\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "batch_size = 20\n",
    "\n",
    "csv_file = './../PrimateRobustness/train_data.csv'\n",
    "dataloader = get_dataloader(csv_file, root_dir, batch_size, transform)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 3\n",
    "initial_lr = 0.001\n",
    "min_lr = 0.0001\n",
    "decay_factor = 0.1\n",
    "patience = 3 \n",
    "criterion1 = nn.MSELoss()\n",
    "criterion2 = nn.MSELoss()\n",
    "optimizer1 = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "optimizer2 = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "\n",
    "lr_scheduler1 = optim.lr_scheduler.ReduceLROnPlateau(optimizer1, mode='min', factor=decay_factor, patience=patience, min_lr=min_lr)\n",
    "lr_scheduler2 = optim.lr_scheduler.ReduceLROnPlateau(optimizer2, mode='min', factor=decay_factor, patience=patience, min_lr=min_lr)\n",
    "learning_rate = initial_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the trained model\n",
    "model_path = \"./trained_two_tailed_resnet50_lr_{}_epochs_{}.pth\".format(learning_rate, num_epochs)\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "# Load the saved model\n",
    "saved_model = modify_resnet50(device)\n",
    "saved_model = nn.DataParallel(saved_model)\n",
    "saved_model.load_state_dict(torch.load(model_path))\n",
    "saved_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# Training loop\n",
    "scaler = GradScaler(device)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    batch_num = 0\n",
    "    for images, (labels1, labels2) in dataloader:\n",
    "        batch_num += 1\n",
    "        # Forward pass\n",
    "        with autocast(device):\n",
    "            model.module.freeze(False)\n",
    "            model.module.freeze_fc2(False)\n",
    "            outputs = model(images.to(device))\n",
    "            outputs1 = outputs[0]\n",
    "            outputs2 = outputs[1].squeeze()\n",
    "            \n",
    "            loss1 = criterion1(outputs1.float(), labels1.float().to(device))\n",
    "            loss2 = criterion2(outputs2.float(), labels2.float().to(device))\n",
    "        if batch_num>1000:\n",
    "            model.module.freeze_fc2(True)\n",
    "            \n",
    "            optimizer1.zero_grad()\n",
    "            loss1.backward(retain_graph = True)\n",
    "            optimizer1.step()\n",
    "            model.module.freeze(True)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            model.module.freeze(True)\n",
    "            optimizer2.zero_grad()\n",
    "            scaler.scale(loss2).backward()\n",
    "            scaler.step(optimizer2)\n",
    "            scaler.update()\n",
    "            lr_scheduler2.step(loss2.item())\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if batch_num%3000==0:\n",
    "            initial_lr = initial_lr*5\n",
    "            optimizer1 = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "\n",
    "\n",
    "        current_loss = loss1.item() + loss2.item()\n",
    "        running_loss += current_loss\n",
    "        \n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        wandb.log({\"loss1\": loss1.item(), \"loss2\": loss2.item()})\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directory pathsClean hf source\n",
    "base_dir = './../primate_robustness_mess/PrimateVisionRobustness/Data/Robustness/Original images/'\n",
    "subfolders = ['Clean hf source', 'Clean mf source']\n",
    "\n",
    "# List to store the data\n",
    "data = []\n",
    "\n",
    "# Iterate through subfolders and files\n",
    "for subfolder in subfolders:\n",
    "    folder_path = os.path.join(base_dir, subfolder)\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(('.png', '.jpg', '.jpeg')):  # Add other image extensions if needed\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            mean_value = 0 if subfolder == 'Clean mf source' else 1\n",
    "            data.append(['clean', mean_value, 'clean', 0, mean_value, 1, file_path])\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data, columns=['SVM', 'direction', 'category', 'level', 'mean', 'dist', 'filename']).reset_index().reset_index()\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('clean_images_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DataLoader\n",
    "def get_dataloader(csv_file, root_dir, batch_size, transform):\n",
    "    dataset = CustomDataset(csv_file=csv_file, root_dir=root_dir, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "# Model Modification\n",
    "def modify_resnet50(num_classes1, num_classes2):\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "\n",
    "    # Modify the fully connected layer to output two sets of classes\n",
    "    class CustomResNet50(nn.Module):\n",
    "        def __init__(self, original_model, num_classes1):\n",
    "            super(CustomResNet50, self).__init__()\n",
    "            self.features = nn.Sequential(*list(original_model.children())[:-1])\n",
    "            self.fc1 = nn.Sequential(\n",
    "                nn.Linear(num_ftrs, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.4),\n",
    "                nn.Linear(512, 2)\n",
    "            )\n",
    "            self.fc2 = nn.Sequential(\n",
    "                nn.Linear(num_ftrs, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 1),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.features(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            out1 = self.fc1(x)\n",
    "            out2 = self.fc2(x)\n",
    "            return out1, out2\n",
    "\n",
    "    model = CustomResNet50(model, num_classes1)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters\n",
    "    csv_file = 'path/to/annotations.csv'\n",
    "    root_dir = 'path/to/images'\n",
    "    batch_size = 32\n",
    "    num_classes1 = 10\n",
    "    num_classes2 = 5\n",
    "\n",
    "    # Transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = get_dataloader(csv_file, root_dir, batch_size, transform)\n",
    "\n",
    "    # Model\n",
    "    model = modify_resnet50(num_classes1, num_classes2)\n",
    "\n",
    "    # Print model architecture\n",
    "    print(model)\n",
    "    # Training parameters\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, (labels1, labels2) in dataloader:\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            outputs1 = outputs[:, :num_classes1]\n",
    "            outputs2 = outputs[:, num_classes1:]\n",
    "            \n",
    "            loss1 = criterion(outputs1, labels1)\n",
    "            loss2 = criterion(outputs2, labels2)\n",
    "            # Check if the losses are within a ratio of 50\n",
    "            if loss1 > 50 * loss2:\n",
    "                loss1 = 50 * loss1\n",
    "            elif loss2 > 50 * loss1:\n",
    "                loss2 = 50 * loss2\n",
    "\n",
    "            # Backward pass for loss1 and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss1.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Backward pass for loss2 and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss2.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += (loss1.item() + loss2.item())\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "    print(\"Finished Training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monke_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
